---
title: "ニューラルネットワークによる関数の近似"
date: 2019-05-20T00:10:18+09:00
draft: false
type: post
---

自分の勉強のためfortranで最も単純なニューラルネットワークを作った。  

(2019/6/23 追記：TensorFlowがニューラルネットワークの仕組みをブラウザ上でインタラクティブに体感できるツールを提供している(https://playground.tensorflow.org) 。  
以下に書いたような内容に近いことを扱っているが、もちろん、ずっと洗練されている。)

ニューラルネットワークを回帰問題に用いる場合、入力変数と出力変数の間の任意の関係を表現する、汎用的な近似器として用いることができるとされる。  

関数形を何も措定せずに、非線形を含むどんな形の関数関係でも近似できるというのは、最初に聞いたときはとても違和感があった。
しかし、その仕組みを知ると、実際のところとても自然な方法であるとわかった。
自然というのは、人間の認識の仕方によく似ているということである。

最も簡単な、入力と出力の変数が1個ずつ(xとyとする)の場合を考えてみる。  
下に適当に作ったxとyのデータを示す。この点の集まりからxとyの間の関係についての推定を行うことを考える。  

<figure style="display:table;">
<image src="/~amemiya/images/post/190520/points.png" alt="data" width="360px" style="clear:both; text-align:center;" margin="50px" >
<figcaption float="center">データ</figcaption>
</figure>

ぱっと見でこの図をどう捉えるか？
仮に最小限の言葉で特徴を説明する必要があるとしたら、例えば次のような説明のしかたができると思われる。  
- 左に山がある  
- その山の右肩により小さな山がある  
- その山の右側に大きな谷がある  

もしくははサイクリングの行程表のように、左から右に進むときの標高の変化を表していると考えることにして、次のように書くこともできる。  
- 初めに少し上り  
- 長いゆるやかな下り  
- 少し上り  
- 急な短い下り  
- しばらく平坦ののち、急な登り

いずれの場合も、多くの点からなる情報を少ない数の要素に分割して説明している。
基本的な要素は、前者の場合は山と谷、後者の場合は上りと下りからなる。
いずれにしても、図をぱっと見たときに人間の頭の中で最初に行われることは、
全体が大体いくつの要素からなり、それぞれ大きさや位置がどのくらいであるかを把握することだと思われる。

一方で、統計的なデータ解析も基本的に目指すことは同じである。すなわち多くの要素からなるデータの特徴を少ない言葉で記述することが目的である。
データ解析においてはどのように要素への分解を行っているだろうか。
単純な線形回帰の場合を考えてみる。

回帰問題は下のように書け、\\(K\\)個のパラメタ\\(w\_k\\)および\\(b\\)を求める問題となる。
$$ y(x) = \sum_{k=1}^{K} w_k \sigma_k (x) +b $$
ここで\\( \sigma_k \\)は基底関数(basis function)であり、その選び方は問題に応じて様々である。  
例として、多項式、フーリエ級数、ガウシアンの基底関数の組をそれぞれ下に示す。

<figure style="display:table; clear:both;">
<image src="/~amemiya/images/post/190520/basis_poly.png" alt="b_poly" width="240px" margin="20px" >
<image src="/~amemiya/images/post/190520/basis_Fourier.png" alt="b_four" width="240px" margin="20px" >
<image src="/~amemiya/images/post/190520/basis_exp.png" alt="b_expy" width="240px" margin="20px" >
<figcaption float="center">基底関数</figcaption>
</figure>

線形回帰問題では、基底関数（および定数）の線形結合で目的の関数を近似する。これを図にすると下のようになる。  
左が入力データ\\(x\\)、右が出力データ\\(y\\)であり。中間の層の〇が基底関数、右側の線が係数\\(w,\ b\\)に対応する。

<figure style="display:table;">
<image src="/~amemiya/images/post/190520/pic_reg.png" alt="poly" width="240px" style="clear:both; text-align:center;" margin="50px" >
<figcaption float="left">線形回帰 多項式の場合</figcaption>
</figure>


6次までの多項式で回帰する場合、結果は下の図のようになる。
点線がそれぞれの基底関数であり（ただし便宜上縦にずらしている）、それらの総和が実線で表された回帰式となる。

<figure style="display:table;">
<image src="/~amemiya/images/post/190520/poly.png" alt="poly" width="360px"  style="clear:both; text-align:center;" margin="50px" >
<figcaption float="left">6次の多項式による回帰</figcaption>
</figure>

6個のフーリエ級数の場合は下の図のようになる。
<figure style="display:table;">
<image src="/~amemiya/images/post/190520/fourier.png" alt="four" width="360px"  style="clear:both; text-align:center;" margin="50px" >
<figcaption float="left">フーリエ級数による回帰</figcaption>
</figure>

いずれの場合も図の右端付近で余計な曲線が入っており近似の精度が落ちている。これは基底関数が範囲全体にわたって変化するため、
右端の崖のような局所的な特徴が表現しづらいためである。

他に、基底関数としてガウシアンを用いることも考えられる。これは緩やかな移動平均（＝ローパスフィルタ）の意味を持つ。
この場合は大きくずれることはないが、代わりに平均する幅より小さな構造がぼやけてしまうと予想される。

もっと先ほど述べたような人間の直感的な要素分割に似た基底関数の選び方はないだろうか。  
ガウシアンを基底関数として用いることは「山と谷」の組み合わせによる表現に近い。
しかし基底関数として用いるには、どの位置のどの幅の山を用意しておけばよいか？という問題が生じる。
任意の状況に対応できるには、できるだけ多くの様々な位置と幅のガウシアンを基底関数として準備し、
それぞれについての回帰係数を多数求めることになるが、これでは少ない要素で説明するという当初の目的から離れてしまう。
「上りと下り」を基底関数とする場合も同様である。

そこで、基底関数として与える「山や谷/上りや下り」の位置や幅自体もデータに合わせて決めることにする。これなら少ない数の基底関数で様々な状況に対応できる。  
具体的には、基底関数の関数形自体は与え、ただしその\\(x\\)方向の位置と大きさ（スケーリングファクター）は可変とする。式で書くと次のようになる。  
$$ y(x) = \sum_{k=1}^{K} w^{(2)}_k \sigma(z_k) +b^{(2)} $$
$$ z_k = w^{(1)}_k x + b^{(1)}_k $$

このようにして用いる関数\\(\sigma\\)は、基底関数のかわりに「活性化関数」とよばれる。  
活性化関数\\(\sigma\\)としてtanh(tangent hyperbolic)を用いることにする。tanhの形状は下の図の通り、緩やかな階段状関数である。
つまり「上り/下り」の要素の一単位を表現するのに向いた関数である。
<figure style="display:table;">
<image src="/~amemiya/images/post/190520/basis_tanh_2.png" alt="b_tanh" width="360px" style="clear:both; text-align:center;" margin="50px" >
<figcaption text-align:center;">tanh</figcaption>
</figure>

二つの係数の組、および活性化関数と入出力データの関係を図にすると下のようになる。  


<figure style="display:table;">
<image src="/~amemiya/images/post/190520/pic_nn.png" alt="poly" width="240px"  style="clear:both; text-align:center;" margin="50px" >
<figcaption text-align:center;">2層ニューラルネットワークを用いた回帰</figcaption>
</figure>

これが、（おそらく）考えられる限り最も単純な、非線形の活性化関数をもつニューラルネットワークの構造である。  
入出力データの多数の組を与え、この構造に従って繰り返し計算によって損失関数を最小にするような係数を求めることで、データから推測される関数を近似することができる。  
（どのようにすれば正しく効率よく学習できるかについては、今回のような単純な例であっても自明ではない問題だが、ここでは省略する）。  
先の例をニューラルネットワークを用いて、十分な学習を行ったあとに得られた回帰曲線とそのときの基底関数\\(\sigma(z\_k)\\)を下に示す。

<figure style="display:table;">
<image src="/~amemiya/images/post/190520/nn.png" alt="poly" width="360px" style="clear:both; text-align:center;" margin="50px" >
<figcaption style="text-align:center;">2層のニューラルネットワークによる回帰</figcaption>
</figure>


基底関数はいずれもtanhであるが、その位置と幅はそれぞれ全く異なっている。
したがって、各々の基底関数は「ある幅の、ある位置の」上りまたは下りの要素を表していることになる。
それらを線形結合した回帰曲線は、複数の上り下りを含む全体のふるまいを適切に表現している。

ここまで来れば、ニューラルネットワークがいかにして任意の非線形の関数の近似において、
人間の直感にとても似た操作をしていることが理解できるのではないだろうか。  
グローバルな関数の重ね合わせとして捉えるのではなく、領域を区切って要素ごとに分解する。ただしその領域の区切り方や要素の種類の選び方は状況に応じて柔軟に行われる。  

何か特別な発想の飛躍によって実現しているわけではなく、関数関係を認識して表現する際の必然的な思考に沿ったとても自然な操作であり、  
巧妙な技術というよりむしろ正面から力業で押していくような方法という印象がある。

ここでは最も単純な1変数同士の関係の例しか見ていないが、多次元の場合や多層の場合も本質的には同じだと思う。  
多層の場合に具体的にどのようにして多様な関数の表現が可能になるのかは、いわゆるディープラーニングを理解するために大事な点のような気がする。 それについてはまた考えてみたい。



